{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "####导入数据，找到配对股票\n",
    "# import costF\n",
    "from random import uniform\n",
    "from random import randint \n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from jqdatasdk import *\n",
    "import pandas as pd             \n",
    "from numpy import nan            \n",
    "import statsmodels.api as sm           \n",
    "import seaborn as sns\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# In[3]:\n",
    "\n",
    "def create_dataset(dataset, look_back):\n",
    "    dataX,dataY=[],[]\n",
    "#     print('dataset',dataset)\n",
    "#     print(len(dataset))\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        x=dataset[i:i+look_back,0]\n",
    "        dataX.append(x)\n",
    "        y=dataset[i+look_back,0]\n",
    "        dataY.append(y)\n",
    "        print('X:%s,Y:%s'%(x,y))\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "\n",
    "def build_model(nest):\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(units=int(nest[4]),input_shape=(1,int(nest[2])),return_sequences=True))\n",
    "    model.add(Dropout(nest[5]))\n",
    "    model.add(LSTM(units=int(nest[6]),return_sequences=False))\n",
    "    model.add(Dropout(nest[7]))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def fitness(nest):\n",
    "    # 获取沪深300指数成分股 前复权后的历史股价 副本1.csv \n",
    "    index = '000300.XSHG'\n",
    "    start = '2017-11-1'\n",
    "    end   = '2019-10-31'\n",
    "    # auth('18827513013','513013')\n",
    "    # auth('19802176017','Limin123')\n",
    "    # code_list = get_index_stocks(index)\n",
    "    # index_df = get_price(code_list, start, end, fields='close').close\n",
    "    # index_df.to_csv('./副本1.csv')\n",
    "    # index_df.head()\n",
    "\n",
    "    index_df = pd.read_csv('./aa.csv', index_col= 0, header= 0)\n",
    "\n",
    "    len(index_df)   #总共有487行数据\n",
    "\n",
    "    df=index_df.dropna(axis = 1)    #axis = 1代表删除缺失值所在的列，这个案例里，删除了有缺失值的股票代码    \n",
    "    df.to_csv('./副本2.csv')             #dropna默认删除缺失值所在的行，也就是默认axis=0\n",
    "    df.head()\n",
    "\n",
    "    np.isnan(df).any().sum() #检查数据集里是否还有缺失值\n",
    "\n",
    "    df_log=np.log(df)   #对数据取对数\n",
    "    df_log.to_csv('./副本对数.csv')\n",
    "    df_log.head()\n",
    "\n",
    "    train=df_log.loc[\"2017-11-1\":\"2018-10-31\"]#2017-11-01至2018-10-31为训练+验证数据，共244个数据（先用这部分数据选出相关系数高的股票对）  \n",
    "    train.to_csv('./副本train.csv')         \n",
    "    train.head()\n",
    "    train.tail()\n",
    "\n",
    "    len(train)\n",
    "\n",
    "    test=df_log.loc[\"2018-11-1\":,]   #用来预测\n",
    "    test.head()\n",
    "\n",
    "    len(test)\n",
    "\n",
    "    # 计算每支股票与其余股票的相关系数，降序排列corr_df = train.corr()\n",
    "    # 相关系数相等的两支股票即为配对股票corr_df[corr_df==1] = nan\n",
    "    # 没有组成配对的股票是由于它与已组成配对的股票的相关系数低于已组成的股票对的相关系数\n",
    "    corr_df = train.corr()\n",
    "    corr_df[corr_df==1] = nan\n",
    "    corr_df = pd.DataFrame(corr_df.max().sort_values(ascending=False).head(10), columns=['corr'])\n",
    "    # corr_df['name'] = [get_security_info(code).display_name for code in corr_df.index]\n",
    "    # corr_df['industry'] = [get_industry(code) for code in corr_df.index] \n",
    "    corr_df.to_csv('./副本配对股票1.csv')\n",
    "    corr_df                    #得到配对股票\n",
    "    ####检验配对股票的协整关系\n",
    "\n",
    "    # GY=train['000728.XSHE'] #取对数后的train训练集中国元证券和东吴证券走势相近\n",
    "    # DW=train['601555.XSHG']  #东吴证券\n",
    "    # plot(GY);plot(DW)\n",
    "    # plt.xlabel('Time');plt.ylabel('LOG(Price)')\n",
    "    # plt.legend(['000728.XSHE'],['601555.XSHG'],loc='best')\n",
    "\n",
    "    #综上可以看到  东吴证券和长江证券一阶单整  可以做协整检验\n",
    "    #协整检验  先做一元回归   再对残差做单位根检验\n",
    "    x = train['000728.XSHE']\n",
    "    y = train['601555.XSHG']\n",
    "    X = sm.add_constant(x)\n",
    "    result = (sm.OLS(y,X)).fit()\n",
    "    print(result.summary())\n",
    "\n",
    "    plot(y-0.8858*x);              #根据ols的估计系数，画出y-0.8858*x的平稳序列\n",
    "    # plt.axhline((y-0.8858*x).mean(), color=\"red\", linestyle=\"--\")\n",
    "    # plt.xlabel(\"Time\"); plt.ylabel(\"Stationary Series\")\n",
    "    # plt.legend([\"Stationary Series\", \"Mean\"])\n",
    "\n",
    "    from statsmodels.tsa.stattools import adfuller        #单位根检验P<0.05 说明平稳\n",
    "    adftest=adfuller(y-0.8858*x-0.1575)                   #东吴证券和国元证券之间存在着长期的均衡关系\n",
    "    result=pd.Series(adftest[0:4],index=['Test Statistic','p-value','Lags Used','Numbers of Observations Used'])\n",
    "    for key,value in adftest[4].items():\n",
    "        result['Critical Value (%s)'%key]=value\n",
    "    print(result)                                 #残差的单位根检验\n",
    "\n",
    "    spread=y-0.8858*x-mean(y-0.8858*x)\n",
    "    plot(spread)\n",
    "    spread.head()\n",
    "\n",
    "\n",
    "    spread.size\n",
    "\n",
    "    # In[30]:\n",
    "    seed=7\n",
    "    batch_size=int(nest[0])\n",
    "    epochs=int(nest[1])    #迭代10次\n",
    "    filename=spread\n",
    "    footer=3\n",
    "    look_back=int(nest[2])   #用前十次的数据来预测下一时刻的数据 \n",
    "    lr=nest[3]\n",
    "\n",
    "    #设置随机数种子\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #导入数据\n",
    "    data=filename\n",
    "    dataset=data.values.astype('float64')\n",
    "    \n",
    "    #标准化数据\n",
    "    scaler=MinMaxScaler()\n",
    "    dataset=scaler.fit_transform(dataset.reshape(-1,1))\n",
    "    train_size=len(dataset[0:200])    \n",
    "    validation_size=len(dataset)-train_size\n",
    "    train,validation=dataset[0: train_size, :],dataset[train_size:len(dataset), :]\n",
    "    \n",
    "    #创建dataset,让数据产生相关性\n",
    "    X_train,y_train=create_dataset(train, int(nest[2]))\n",
    "    X_validation,y_validation=create_dataset(validation, int(nest[2]))\n",
    "    \n",
    "    #将输入转化成【样本，时间步长，特征】\n",
    "    X_train=np.reshape(X_train,(X_train.shape[0],1,X_train.shape[1]))\n",
    "    X_validation=np.reshape(X_validation,(X_validation.shape[0],1,X_validation.shape[1]))\n",
    "    \n",
    "    #训练模型\n",
    "    model=build_model(nest)\n",
    "    model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,verbose=2,validation_data=(X_validation,y_validation))\n",
    "    \n",
    "    #模型预测数据\n",
    "    predict_train=model.predict(X_train)\n",
    "    predict_validation=model.predict(X_validation)\n",
    "    \n",
    "    #反标准化数据，目的是保证MSE的准确性\n",
    "    predict_train=scaler.inverse_transform(predict_train)\n",
    "    y_train=scaler.inverse_transform([y_train])\n",
    "    predict_validation=scaler.inverse_transform(predict_validation)\n",
    "    y_validation=scaler.inverse_transform([y_validation])\n",
    "    \n",
    "    #评估模型\n",
    "    train_score=math.sqrt(mean_squared_error(y_train[0],predict_train[:, 0]))\n",
    "    print('Train Score: %.2f RMSE' % train_score)\n",
    "    validation_score=math.sqrt(mean_squared_error(y_validation[0],predict_validation[:, 0]))\n",
    "    print('Validation Score: %.2f RMSE' % validation_score)\n",
    "    \n",
    "    #构建通过训练数据集进行预测的图表数据\n",
    "    predict_train_plot=np.empty_like(dataset)\n",
    "    predict_train_plot[:, :]=np.nan\n",
    "    predict_train_plot[look_back:len(predict_train)+look_back, :]=predict_train\n",
    "    \n",
    "    #构建通过评估数据集进行预测的图表数据\n",
    "    predict_validation_plot=np.empty_like(dataset)\n",
    "    predict_validation_plot[:, :]=np.nan\n",
    "    predict_validation_plot[len(predict_train)+look_back*2+1:len(dataset)-1, :]=predict_validation\n",
    "    \n",
    "    #图表显示\n",
    "    dataset=scaler.inverse_transform(dataset)\n",
    "    # plt.plot(dataset,color='blue')\n",
    "    # plt.plot(predict_train_plot,color='green')\n",
    "    # plt.plot(predict_validation_plot,color='red')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # In[40]:\n",
    "\n",
    "\n",
    "    fx=test['000728.XSHE']            #测试集的价差\n",
    "    fy=test['601555.XSHG']\n",
    "    fspread=fy-0.8858*fx-mean(fy-0.8858*fx)\n",
    "\n",
    "\n",
    "    # In[41]:\n",
    "\n",
    "\n",
    "    XY=fspread         #根据训练好的模型进行预测\n",
    "    dataset=XY.values.astype('float64')\n",
    "    #标准化数据\n",
    "    scaler=MinMaxScaler()\n",
    "    dataset=scaler.fit_transform(dataset.reshape(-1,1))\n",
    "        \n",
    "    #创建dataset,让数据产生相关性\n",
    "    X_test,y_test=create_dataset(dataset, int(nest[2]))\n",
    "        \n",
    "    #将输入转化成【样本，时间步长，特征】\n",
    "    X_test=np.reshape(X_test,(X_test.shape[0],1,X_test.shape[1]))\n",
    "\n",
    "\n",
    "    #模型预测数据\n",
    "    predict_test=model.predict(X_test)\n",
    "        \n",
    "    #反标准化数据，目的是保证MSE的准确性\n",
    "    predict_test=scaler.inverse_transform(predict_test)\n",
    "    y_test=scaler.inverse_transform([y_test])\n",
    "        \n",
    "    #评估模型\n",
    "    test_score=math.sqrt(mean_squared_error(y_test[0],predict_test[:, 0]))\n",
    "    print('Test Score: %.2f RMSE' % test_score)\n",
    "\n",
    "    #构建通过测试数据集进行预测的图表数据\n",
    "    predict_test_plot=np.empty_like(dataset)\n",
    "    predict_test_plot[:, :]=np.nan\n",
    "    predict_test_plot[look_back:len(predict_test)+look_back, :]=predict_test\n",
    "\n",
    "    #图表显示\n",
    "    dataset=scaler.inverse_transform(dataset)\n",
    "    # plt.plot(dataset,color='blue')\n",
    "    # plt.plot(predict_test_plot,color='red')\n",
    "    # plt.show()\n",
    "\n",
    "    return validation_score\n",
    "\n",
    "'''\n",
    "根据levy飞行计算新的巢穴位置\n",
    "'''\n",
    "def GetNewNestViaLevy(Xt,Xbest,Lb,Ub,lamuda):\n",
    "    beta = 1.5\n",
    "    sigma_u = (math.gamma(1 + beta) * math.sin(math.pi * beta / 2) / (\n",
    "                math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n",
    "    sigma_v = 1\n",
    "    for i in range(Xt.shape[0]):\n",
    "        s = Xt[i,:]\n",
    "        u = np.random.normal(0, sigma_u, 1)\n",
    "        v = np.random.normal(0, sigma_v, 1)\n",
    "        Ls = u / ((abs(v)) ** (1 / beta))\n",
    "        stepsize = lamuda*Ls*(s-Xbest)   #lamuda的设置关系到点的活力程度  方向是由最佳位置确定的  有点类似PSO算法  但是步长不一样\n",
    "        s = s + stepsize * np.random.randn(1, len(s))  #产生满足正态分布的序列\n",
    "        Xt[i, :] = s\n",
    "        Xt[i,:] = simplebounds(s,Lb,Ub)\n",
    "    return Xt\n",
    "'''\n",
    "按pa抛弃部分巢穴\n",
    "'''\n",
    "def empty_nests(nest,Lb,Ub,pa):\n",
    "    n = nest.shape[0]\n",
    "    nest1 = nest.copy()\n",
    "    nest2 = nest.copy()\n",
    "    rand_m =pa - np.random.rand(n,nest.shape[1])\n",
    "    rand_m = np.heaviside(rand_m,0)\n",
    "    np.random.shuffle(nest1)\n",
    "    np.random.shuffle(nest2)\n",
    "    # stepsize = np.random.rand(1,1) * (nest1 - nest)\n",
    "    stepsize = np.random.rand(1,1) * (nest1 - nest2)\n",
    "    new_nest = nest + stepsize * rand_m\n",
    "    nest = simplebounds(new_nest,Lb,Ub)\n",
    "    return nest\n",
    "'''\n",
    "获得当前最优解\n",
    "'''\n",
    "def get_best_nest(nest, newnest,Nbest,nest_best):\n",
    "    fitall = 0\n",
    "    for i in range (nest.shape[0]):\n",
    "        temp1 = fitness(nest[i,:])\n",
    "        temp2 = fitness(newnest[i,:])\n",
    "        if temp1 > temp2:\n",
    "            nest[i, :] = newnest[i,:]\n",
    "            if temp2 < Nbest :\n",
    "                Nbest = temp1\n",
    "                nest_best = nest[i,:]\n",
    "            fitall = fitall + temp2\n",
    "        else:\n",
    "            fitall = fitall + temp1\n",
    "    meanfit = fitall/nest.shape[0]\n",
    "    return  nest , Nbest , nest_best ,meanfit\n",
    "\n",
    "'''\n",
    "约束迭代结果\n",
    "'''\n",
    "def simplebounds(s,Lb,Ub):\n",
    "    for i in range(s.shape[0]):\n",
    "        for j in range(s.shape[1]):\n",
    "            if s[i][j] < Lb[j]:\n",
    "                s[i][j] = Lb[j]\n",
    "            if s[i][j] > Ub[j]:\n",
    "                s[i][j] = Ub[j]\n",
    "    return s\n",
    " \n",
    "def Get_CS(Lb,Ub,maxgen):\n",
    "    '''\n",
    "    Lb - 优化变量下边界 eg. Lb = [-1, -1, -0.1]\n",
    "    Ub - 优化变量上边界 eg. Ub = [3, 5, 10]\n",
    "    maxgen - 迭代次数\n",
    "    '''\n",
    "\n",
    "    population_size = 20 # 布谷鸟种群数\n",
    "    lamuda = 1\n",
    "    pa =0.25\n",
    "    costfmin = []\n",
    "    bestpop = []\n",
    "    dim = len(Lb)\n",
    "    nest = np.array(Lb) + (np.array(Ub)-np.array(Lb))*np.random.uniform(0, 1,(population_size,dim))  # 初始化位置\n",
    "    nest_best = nest[0,:]\n",
    "    Nbest =fitness(nest_best)\n",
    "    nest ,Nbest, nest_best ,fitmean = get_best_nest(nest,nest,Nbest,nest_best)\n",
    "    for i in range (maxgen):\n",
    "        nest_c = nest.copy()\n",
    "        newnest = GetNewNestViaLevy(nest_c, nest_best, Lb, Ub,lamuda) # 根据莱维飞行产生新的位置\n",
    " \n",
    "        nest,Nbest, nest_best ,fitmean = get_best_nest(nest, newnest,Nbest, nest_best) # 判断新的位置优劣进行替换\n",
    " \n",
    "        nest_e = nest.copy()\n",
    "        newnest = empty_nests(nest_e,Lb,Ub,pa) #丢弃部分巢穴\n",
    " \n",
    "        nest,Nbest, nest_best ,fitmean = get_best_nest(nest,newnest, Nbest, nest_best)  # 再次判断新的位置优劣进行替换\n",
    " \n",
    "        costfmin.append(Nbest)\n",
    "        bestpop.append(nest_best)\n",
    "        print(\"第\",i,\"次迭代，最优解的适应度函数值\",Nbest)\n",
    "\n",
    "    return  costfmin, bestpop\n",
    "\n",
    "\n",
    "# Lb = [-10,-10]\n",
    "# Ub = [10,10]\n",
    "\n",
    "# Lb = [1, 10, 5, 0, 10, 0, 10, 0]\n",
    "# Ub = [300, 500, 20, 1, 200, 1, 200, 1]\n",
    "\n",
    "# maxgen = 30\n",
    "# costfmin, bestpop = Get_CS(Lb=Lb,Ub=Ub,maxgen=maxgen)\n",
    "# plt.plot(costfmin)\n",
    "# plt.xlabel('time')\n",
    "# plt.ylabel('cost function')\n",
    "# plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # batch_size = 1\n",
    "    # epochs = 20    #迭代10次\n",
    "    # look_back = 10   #用前十次的数据来预测下一时刻的数据 \n",
    "    # lr = 0.0006\n",
    "    # units, drop_out= 10, 0.5\n",
    "    # units_1, drop_out_1 = 10, 0.5\n",
    "    # nest = [batch_size, epochs, look_back, lr, units, drop_out, units_1, drop_out_1]\n",
    "    # fitness(nest)\n",
    "\n",
    "\n",
    "    Lb = [1, 10, 5, 0, 10, 0, 10, 0]\n",
    "    Ub = [300, 500, 20, 1, 200, 1, 200, 1]\n",
    "\n",
    "    maxgen = 30\n",
    "    costfmin, bestpop = Get_CS(Lb=Lb,Ub=Ub,maxgen=maxgen)\n",
    "    print('costfmin:', costfmin)\n",
    "    plt.plot(costfmin)\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('cost function')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
